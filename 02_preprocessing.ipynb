{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281b34e3",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è DATA PREPROCESSING\n",
    "\n",
    "## Stage 2: Clean, Transform, and Prepare Data\n",
    "\n",
    "Welcome to the preprocessing stage! This notebook covers essential data preparation techniques:\n",
    "\n",
    "- **Data Cleaning**: Handle missing values, duplicates, and outliers\n",
    "- **Data Transformation**: Scaling, normalization, and mathematical transformations\n",
    "- **Feature Engineering**: Create new features, encode categoricals, and binning\n",
    "- **Dimensionality Reduction**: PCA, t-SNE, UMAP for feature reduction\n",
    "\n",
    "Let's build a comprehensive preprocessing pipeline using our FIFA dataset! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb067a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "plt.style.use('default')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Preprocessing libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FIFA dataset\n",
    "data = pd.read_csv(\"/Users/gabi/Desktop/DA_practice/Datasets/fifa_eda.csv\")\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "data.info()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "missing_summary = data.isnull().sum()\n",
    "missing_percentage = (missing_summary / len(data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_summary,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48066d",
   "metadata": {},
   "source": [
    "# üîß Data Cleaning\n",
    "\n",
    "## Missing Values Handling\n",
    "\n",
    "Missing values are a common issue in real-world datasets. There are several strategies to handle them:\n",
    "\n",
    "1. **Deletion**: Remove rows or columns with missing values\n",
    "2. **Mean/Median/Mode Imputation**: Fill with central tendency measures\n",
    "3. **Forward/Backward Fill**: Use adjacent values (for time-series)\n",
    "4. **KNN Imputation**: Use k-nearest neighbors to predict missing values\n",
    "5. **Advanced Imputation**: Use machine learning models\n",
    "\n",
    "Let's explore different approaches with our FIFA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f576f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Handling - Multiple Approaches\n",
    "\n",
    "# Create a copy for experimentation\n",
    "df = data.copy()\n",
    "\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Simple deletion approach\n",
    "print(\"1. DELETION APPROACH:\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "df_dropped_rows = df.dropna()\n",
    "print(f\"After dropping rows with ANY missing values: {df_dropped_rows.shape}\")\n",
    "\n",
    "# Drop columns with high missing percentage (>20%)\n",
    "high_missing_cols = missing_df[missing_df['Missing Percentage'] > 20].index\n",
    "print(f\"Columns with >20% missing: {list(high_missing_cols)}\")\n",
    "\n",
    "# 2. Imputation approaches\n",
    "print(\"\\n2. IMPUTATION APPROACHES:\")\n",
    "\n",
    "# Numerical columns imputation\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Mean imputation for numerical\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_mean_imputed = df.copy()\n",
    "df_mean_imputed[numerical_cols] = imputer_mean.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Median imputation for numerical\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df_median_imputed = df.copy()\n",
    "df_median_imputed[numerical_cols] = imputer_median.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Mode imputation for categorical\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df_mode_imputed = df.copy()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df_mode_imputed[col] = imputer_mode.fit_transform(df[[col]]).flatten()\n",
    "\n",
    "print(\"‚úÖ Mean, Median, and Mode imputation completed\")\n",
    "\n",
    "# 3. KNN Imputation (for numerical features)\n",
    "print(\"\\n3. KNN IMPUTATION:\")\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed = df.copy()\n",
    "df_knn_imputed[numerical_cols] = knn_imputer.fit_transform(df[numerical_cols])\n",
    "print(\"‚úÖ KNN imputation completed\")\n",
    "\n",
    "# Compare the results\n",
    "comparison_col = 'Value'  # Column with missing values\n",
    "print(f\"\\nüìä COMPARISON FOR '{comparison_col}' COLUMN:\")\n",
    "print(f\"Original missing count: {df[comparison_col].isnull().sum()}\")\n",
    "print(f\"Mean imputed - Mean: {df_mean_imputed[comparison_col].mean():.2f}\")\n",
    "print(f\"Median imputed - Median: {df_median_imputed[comparison_col].median():.2f}\")\n",
    "print(f\"KNN imputed - Mean: {df_knn_imputed[comparison_col].mean():.2f}\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDATION: Use KNN imputation for better preservation of relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206fab2",
   "metadata": {},
   "source": [
    "## Outlier Detection & Treatment\n",
    "\n",
    "Outliers can significantly impact model performance. Common detection methods include:\n",
    "- **Statistical Methods**: Z-score, IQR (Interquartile Range)\n",
    "- **Visual Methods**: Box plots, scatter plots\n",
    "- **Machine Learning**: Isolation Forest, One-Class SVM\n",
    "\n",
    "## Duplicate Detection\n",
    "\n",
    "Duplicates can bias analysis and model training. We'll identify and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection and Treatment\n",
    "\n",
    "# Use the KNN imputed dataset for clean analysis\n",
    "df_clean = df_knn_imputed.copy()\n",
    "\n",
    "print(\"üïµÔ∏è OUTLIER DETECTION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 1. Z-Score Method\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())\n",
    "    return data[z_scores > threshold]\n",
    "\n",
    "# 2. IQR Method  \n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "\n",
    "# Analyze outliers in key numerical columns\n",
    "key_columns = ['Overall', 'Age', 'Value', 'Wage']\n",
    "\n",
    "outlier_summary = []\n",
    "for col in key_columns:\n",
    "    zscore_outliers = len(detect_outliers_zscore(df_clean, col))\n",
    "    iqr_outliers = len(detect_outliers_iqr(df_clean, col))\n",
    "    outlier_summary.append({\n",
    "        'Column': col,\n",
    "        'Z-Score Outliers (>3œÉ)': zscore_outliers,\n",
    "        'IQR Outliers': iqr_outliers,\n",
    "        'Total Values': len(df_clean),\n",
    "        'Z-Score %': f\"{(zscore_outliers/len(df_clean)*100):.2f}%\",\n",
    "        'IQR %': f\"{(iqr_outliers/len(df_clean)*100):.2f}%\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df)\n",
    "\n",
    "# 3. Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Outlier Detection Visualization', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(key_columns):\n",
    "    row = i // 2\n",
    "    column = i % 2\n",
    "    \n",
    "    # Box plot to show outliers\n",
    "    sns.boxplot(y=df_clean[col], ax=axes[row, column])\n",
    "    axes[row, column].set_title(f'{col} - Box Plot (IQR Outliers)')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Treatment options\n",
    "print(\"\\nüõ†Ô∏è OUTLIER TREATMENT OPTIONS:\")\n",
    "print(\"1. Remove outliers (can lose valuable information)\")\n",
    "print(\"2. Cap outliers (winsorizing)\")\n",
    "print(\"3. Transform data (log, square root)\")\n",
    "print(\"4. Keep outliers (if they're meaningful)\")\n",
    "\n",
    "# Example: Winsorizing (capping) outliers\n",
    "def winsorize_outliers(data, column, percentiles=(0.05, 0.95)):\n",
    "    \"\"\"Cap outliers at specified percentiles\"\"\"\n",
    "    lower = data[column].quantile(percentiles[0])\n",
    "    upper = data[column].quantile(percentiles[1])\n",
    "    data[column] = np.clip(data[column], lower, upper)\n",
    "    return data\n",
    "\n",
    "# Apply winsorizing to Value column (has extreme outliers)\n",
    "df_winsorized = df_clean.copy()\n",
    "df_winsorized = winsorize_outliers(df_winsorized, 'Value')\n",
    "\n",
    "print(f\"\\nüìä VALUE COLUMN TREATMENT:\")\n",
    "print(f\"Original max value: {df_clean['Value'].max():,.0f}\")\n",
    "print(f\"Winsorized max value: {df_winsorized['Value'].max():,.0f}\")\n",
    "\n",
    "print(\"‚úÖ Outlier detection and treatment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0006b7f",
   "metadata": {},
   "source": [
    "# üîÑ Data Transformation\n",
    "\n",
    "Data transformation is crucial for machine learning algorithms. Different techniques serve different purposes:\n",
    "\n",
    "## Scaling Techniques\n",
    "- **StandardScaler**: Mean=0, Std=1 (Z-score normalization)\n",
    "- **MinMaxScaler**: Scale to [0,1] range\n",
    "- **RobustScaler**: Uses median and IQR (robust to outliers)\n",
    "\n",
    "## Mathematical Transformations\n",
    "- **Log Transform**: For right-skewed data\n",
    "- **Square Root Transform**: Moderate skewness reduction\n",
    "- **Box-Cox Transform**: Automatic optimal transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation and Scaling\n",
    "\n",
    "# Use cleaned dataset\n",
    "df_transform = df_clean.copy()\n",
    "\n",
    "print(\"üîÑ DATA SCALING TECHNIQUES\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Select numerical columns for scaling\n",
    "numerical_features = ['Age', 'Overall', 'Potential', 'Value', 'Wage', 'Height', 'Weight']\n",
    "sample_data = df_transform[numerical_features].copy()\n",
    "\n",
    "# 1. Standard Scaling (Z-score normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "data_standard = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(sample_data),\n",
    "    columns=[f'{col}_standard' for col in numerical_features]\n",
    ")\n",
    "\n",
    "# 2. Min-Max Scaling (0-1 normalization)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data_minmax = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(sample_data),\n",
    "    columns=[f'{col}_minmax' for col in numerical_features]\n",
    ")\n",
    "\n",
    "# 3. Robust Scaling (using median and IQR)\n",
    "robust_scaler = RobustScaler()\n",
    "data_robust = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(sample_data),\n",
    "    columns=[f'{col}_robust' for col in numerical_features]\n",
    ")\n",
    "\n",
    "# Compare scaling results\n",
    "print(\"üìä SCALING COMPARISON FOR 'VALUE' COLUMN:\")\n",
    "print(f\"Original - Mean: {sample_data['Value'].mean():.2f}, Std: {sample_data['Value'].std():.2f}\")\n",
    "print(f\"Standard - Mean: {data_standard['Value_standard'].mean():.2f}, Std: {data_standard['Value_standard'].std():.2f}\")\n",
    "print(f\"MinMax - Min: {data_minmax['Value_minmax'].min():.2f}, Max: {data_minmax['Value_minmax'].max():.2f}\")\n",
    "print(f\"Robust - Median: {data_robust['Value_robust'].median():.2f}, IQR: {data_robust['Value_robust'].quantile(0.75) - data_robust['Value_robust'].quantile(0.25):.2f}\")\n",
    "\n",
    "# 4. Mathematical Transformations\n",
    "print(\"\\nüßÆ MATHEMATICAL TRANSFORMATIONS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Log transformation (for highly skewed data like Value)\n",
    "value_log = np.log1p(df_transform['Value'])  # log1p handles zeros better\n",
    "print(f\"Value - Original skewness: {df_transform['Value'].skew():.2f}\")\n",
    "print(f\"Value - Log transformed skewness: {value_log.skew():.2f}\")\n",
    "\n",
    "# Square root transformation\n",
    "value_sqrt = np.sqrt(df_transform['Value'])\n",
    "print(f\"Value - Sqrt transformed skewness: {value_sqrt.skew():.2f}\")\n",
    "\n",
    "# Box-Cox transformation (requires positive values)\n",
    "from scipy import stats\n",
    "positive_values = df_transform['Value'] + 1  # Ensure positive values\n",
    "value_boxcox, lambda_param = stats.boxcox(positive_values)\n",
    "print(f\"Value - Box-Cox transformed skewness: {pd.Series(value_boxcox).skew():.2f}\")\n",
    "print(f\"Optimal lambda parameter: {lambda_param:.3f}\")\n",
    "\n",
    "# Visualize transformations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Data Transformations for Value Column', fontsize=16)\n",
    "\n",
    "# Original distribution\n",
    "axes[0,0].hist(df_transform['Value'], bins=50, alpha=0.7, color='blue')\n",
    "axes[0,0].set_title(f'Original (Skewness: {df_transform[\"Value\"].skew():.2f})')\n",
    "axes[0,0].set_xlabel('Value')\n",
    "\n",
    "# Log transformation\n",
    "axes[0,1].hist(value_log, bins=50, alpha=0.7, color='green')\n",
    "axes[0,1].set_title(f'Log Transform (Skewness: {value_log.skew():.2f})')\n",
    "axes[0,1].set_xlabel('Log(Value)')\n",
    "\n",
    "# Square root transformation\n",
    "axes[1,0].hist(value_sqrt, bins=50, alpha=0.7, color='orange')\n",
    "axes[1,0].set_title(f'Square Root (Skewness: {value_sqrt.skew():.2f})')\n",
    "axes[1,0].set_xlabel('‚àö(Value)')\n",
    "\n",
    "# Box-Cox transformation\n",
    "axes[1,1].hist(value_boxcox, bins=50, alpha=0.7, color='red')\n",
    "axes[1,1].set_title(f'Box-Cox (Skewness: {pd.Series(value_boxcox).skew():.2f})')\n",
    "axes[1,1].set_xlabel('Box-Cox(Value)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Data transformation techniques demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b13f8",
   "metadata": {},
   "source": [
    "# üîß Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating new features from existing ones to improve model performance:\n",
    "\n",
    "## Categorical Encoding\n",
    "- **Label Encoding**: Convert categories to numbers (ordinal)\n",
    "- **One-Hot Encoding**: Create binary columns for each category\n",
    "- **Target Encoding**: Use target variable statistics\n",
    "\n",
    "## Feature Creation\n",
    "- **Polynomial Features**: Interaction terms and higher-order features\n",
    "- **Binning/Discretization**: Convert continuous to categorical\n",
    "- **Domain-Specific Features**: Business logic-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac340fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Techniques\n",
    "\n",
    "# Create a working copy\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "print(\"üîß FEATURE ENGINEERING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 1. CATEGORICAL ENCODING\n",
    "print(\"1. CATEGORICAL ENCODING:\")\n",
    "\n",
    "# Label Encoding for ordinal categories\n",
    "label_encoder = LabelEncoder()\n",
    "df_features['Preferred_Foot_Encoded'] = label_encoder.fit_transform(df_features['Preferred Foot'])\n",
    "print(f\"‚úÖ Label encoded 'Preferred Foot': {df_features['Preferred Foot'].unique()} -> {df_features['Preferred_Foot_Encoded'].unique()}\")\n",
    "\n",
    "# One-Hot Encoding for nominal categories (top positions only to avoid too many columns)\n",
    "top_positions = df_features['Position'].value_counts().head(10).index\n",
    "df_positions_filtered = df_features[df_features['Position'].isin(top_positions)].copy()\n",
    "\n",
    "position_dummies = pd.get_dummies(df_positions_filtered['Position'], prefix='Position')\n",
    "df_features_encoded = pd.concat([df_positions_filtered, position_dummies], axis=1)\n",
    "print(f\"‚úÖ One-hot encoded top 10 positions: {position_dummies.shape[1]} new columns\")\n",
    "\n",
    "# 2. FEATURE CREATION\n",
    "print(\"\\n2. FEATURE CREATION:\")\n",
    "\n",
    "# Age groups (binning)\n",
    "df_features['Age_Group'] = pd.cut(df_features['Age'], \n",
    "                                 bins=[0, 20, 25, 30, 35, 100], \n",
    "                                 labels=['Very_Young', 'Young', 'Prime', 'Experienced', 'Veteran'])\n",
    "print(\"‚úÖ Created Age Groups based on career stages\")\n",
    "\n",
    "# BMI calculation (domain-specific feature)\n",
    "df_features['BMI'] = df_features['Weight'] / (df_features['Height'] ** 2) * 10000  # Convert to proper BMI\n",
    "print(\"‚úÖ Created BMI from Height and Weight\")\n",
    "\n",
    "# Value per Overall point (efficiency metric)\n",
    "df_features['Value_per_Overall'] = df_features['Value'] / df_features['Overall']\n",
    "df_features['Value_per_Overall'].fillna(0, inplace=True)  # Handle division by zero\n",
    "print(\"‚úÖ Created Value efficiency metric\")\n",
    "\n",
    "# Performance potential (Overall vs Potential gap)\n",
    "df_features['Performance_Gap'] = df_features['Potential'] - df_features['Overall']\n",
    "print(\"‚úÖ Created Performance Gap feature\")\n",
    "\n",
    "# Experience level (years since joined)\n",
    "current_year = 2023  # Approximate current year\n",
    "df_features['Years_Experience'] = current_year - df_features['Joined']\n",
    "print(\"‚úÖ Created Years of Experience\")\n",
    "\n",
    "# 3. POLYNOMIAL FEATURES (interaction terms)\n",
    "print(\"\\n3. POLYNOMIAL FEATURES:\")\n",
    "\n",
    "# Select a subset of numerical features for polynomial expansion\n",
    "base_features = ['Age', 'Overall', 'Height', 'Weight']\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "\n",
    "# Apply to a sample to avoid memory issues\n",
    "sample_size = 1000\n",
    "sample_indices = df_features.sample(n=sample_size, random_state=42).index\n",
    "sample_features = df_features.loc[sample_indices, base_features]\n",
    "\n",
    "poly_features = poly_transformer.fit_transform(sample_features)\n",
    "poly_feature_names = poly_transformer.get_feature_names_out(base_features)\n",
    "\n",
    "print(f\"‚úÖ Created {poly_features.shape[1]} polynomial features from {len(base_features)} base features\")\n",
    "print(f\"New interaction features include: {list(poly_feature_names[len(base_features):len(base_features)+5])}\")\n",
    "\n",
    "# 4. FEATURE SUMMARY\n",
    "print(\"\\nüìä FEATURE ENGINEERING SUMMARY:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "original_features = df_clean.shape[1]\n",
    "new_categorical = 1  # Label encoded Preferred Foot\n",
    "new_binned = 1  # Age groups\n",
    "new_calculated = 5  # BMI, Value_per_Overall, Performance_Gap, Years_Experience\n",
    "new_polynomial = poly_features.shape[1] - len(base_features)  # Only interaction terms\n",
    "\n",
    "print(f\"Original features: {original_features}\")\n",
    "print(f\"+ Categorical encoding: {new_categorical}\")\n",
    "print(f\"+ Binning features: {new_binned}\")  \n",
    "print(f\"+ Calculated features: {new_calculated}\")\n",
    "print(f\"+ Polynomial features: {new_polynomial}\")\n",
    "print(f\"= Total potential features: {original_features + new_categorical + new_binned + new_calculated + new_polynomial}\")\n",
    "\n",
    "# Show examples of new features\n",
    "print(f\"\\nüîç SAMPLE OF NEW FEATURES:\")\n",
    "feature_sample = df_features[['Name', 'Age', 'Age_Group', 'BMI', 'Performance_Gap', 'Years_Experience']].head()\n",
    "print(feature_sample.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc504068",
   "metadata": {},
   "source": [
    "# üìâ Dimensionality Reduction\n",
    "\n",
    "High-dimensional data can suffer from the \"curse of dimensionality\". Reduction techniques help:\n",
    "\n",
    "## Linear Methods\n",
    "- **PCA (Principal Component Analysis)**: Find directions of maximum variance\n",
    "- **LDA (Linear Discriminant Analysis)**: Supervised reduction for classification\n",
    "\n",
    "## Non-Linear Methods  \n",
    "- **t-SNE**: Preserve local structure for visualization\n",
    "- **UMAP**: Faster alternative to t-SNE with global structure preservation\n",
    "\n",
    "These techniques reduce computational cost and can improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Techniques\n",
    "\n",
    "print(\"üìâ DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Prepare numerical features for reduction\n",
    "numerical_cols = ['Age', 'Overall', 'Potential', 'Value', 'Wage', \n",
    "                 'International Reputation', 'Skill Moves', 'Height', 'Weight', 'Release Clause']\n",
    "df_numeric = df_clean[numerical_cols].copy()\n",
    "\n",
    "# Remove any remaining NaN values\n",
    "df_numeric = df_numeric.dropna()\n",
    "\n",
    "# Standardize features (required for PCA)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_numeric)\n",
    "\n",
    "print(f\"Original feature shape: {df_scaled.shape}\")\n",
    "\n",
    "# 1. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "print(\"\\n1. PRINCIPAL COMPONENT ANALYSIS:\")\n",
    "\n",
    "# Apply PCA with different numbers of components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(df_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"‚úÖ Components for 95% variance: {n_components_95}\")\n",
    "print(f\"‚úÖ Components for 99% variance: {n_components_99}\")\n",
    "\n",
    "# Apply PCA with optimal number of components\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "df_pca = pca_optimal.fit_transform(df_scaled)\n",
    "\n",
    "print(f\"‚úÖ Reduced from {df_scaled.shape[1]} to {df_pca.shape[1]} dimensions (95% variance retained)\")\n",
    "\n",
    "# Visualize PCA results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Explained variance plot\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1), pca_full.explained_variance_ratio_)\n",
    "axes[0].set_title('PCA: Explained Variance by Component')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "axes[1].axhline(y=0.99, color='g', linestyle='--', label='99% Variance')\n",
    "axes[1].set_title('PCA: Cumulative Explained Variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# First two principal components scatter plot\n",
    "scatter = axes[2].scatter(df_pca[:, 0], df_pca[:, 1], \n",
    "                         c=df_clean.loc[df_numeric.index, 'Overall'], \n",
    "                         cmap='viridis', alpha=0.6, s=20)\n",
    "axes[2].set_title('PCA: First Two Components')\n",
    "axes[2].set_xlabel('First Principal Component')\n",
    "axes[2].set_ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter, ax=axes[2], label='Overall Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. t-SNE FOR VISUALIZATION\n",
    "print(\"\\n2. t-SNE VISUALIZATION:\")\n",
    "\n",
    "# Sample data for faster t-SNE computation\n",
    "sample_size = 2000\n",
    "if len(df_scaled) > sample_size:\n",
    "    sample_indices = np.random.choice(len(df_scaled), sample_size, replace=False)\n",
    "    df_sample = df_scaled[sample_indices]\n",
    "    sample_overall = df_clean.iloc[df_numeric.index].iloc[sample_indices]['Overall']\n",
    "else:\n",
    "    df_sample = df_scaled\n",
    "    sample_overall = df_clean.loc[df_numeric.index, 'Overall']\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n",
    "df_tsne = tsne.fit_transform(df_sample)\n",
    "\n",
    "print(f\"‚úÖ t-SNE completed on {len(df_sample)} samples\")\n",
    "\n",
    "# 3. COMPARISON VISUALIZATION\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# PCA projection (first 2000 samples to match t-SNE)\n",
    "pca_2d = PCA(n_components=2)\n",
    "df_pca_2d = pca_2d.fit_transform(df_sample)\n",
    "\n",
    "# PCA plot\n",
    "scatter1 = axes[0].scatter(df_pca_2d[:, 0], df_pca_2d[:, 1], \n",
    "                          c=sample_overall, cmap='viridis', alpha=0.6, s=20)\n",
    "axes[0].set_title(f'PCA 2D Projection\\\\n({pca_2d.explained_variance_ratio_.sum():.1%} variance explained)')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Overall Rating')\n",
    "\n",
    "# t-SNE plot\n",
    "scatter2 = axes[1].scatter(df_tsne[:, 0], df_tsne[:, 1], \n",
    "                          c=sample_overall, cmap='viridis', alpha=0.6, s=20)\n",
    "axes[1].set_title('t-SNE 2D Projection\\\\n(Preserves local structure)')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Overall Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. SUMMARY\n",
    "print(\"\\nüìä DIMENSIONALITY REDUCTION SUMMARY:\")\n",
    "print(\"=\"*45)\n",
    "print(f\"Original dimensions: {df_scaled.shape[1]}\")\n",
    "print(f\"PCA (95% variance): {n_components_95} dimensions\")\n",
    "print(f\"PCA (99% variance): {n_components_99} dimensions\")\n",
    "print(f\"t-SNE visualization: 2 dimensions\")\n",
    "print(f\"Compression ratio (95%): {(1 - n_components_95/df_scaled.shape[1])*100:.1f}% reduction\")\n",
    "\n",
    "print(\"‚úÖ Dimensionality reduction techniques completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9eefb",
   "metadata": {},
   "source": [
    "# üéØ Preprocessing Summary\n",
    "\n",
    "## Completed Preprocessing Techniques ‚úÖ\n",
    "\n",
    "We've successfully implemented comprehensive data preprocessing using the FIFA dataset:\n",
    "\n",
    "### **üîß Data Cleaning**\n",
    "- [x] **Missing Values**: Deletion, Mean/Median/Mode imputation, KNN imputation\n",
    "- [x] **Outlier Detection**: Z-score and IQR methods with visualization\n",
    "- [x] **Outlier Treatment**: Winsorizing (capping extreme values)\n",
    "\n",
    "### **üîÑ Data Transformation** \n",
    "- [x] **Scaling**: StandardScaler, MinMaxScaler, RobustScaler\n",
    "- [x] **Mathematical Transforms**: Log, Square Root, Box-Cox transformations\n",
    "- [x] **Distribution Normalization**: Skewness reduction techniques\n",
    "\n",
    "### **üîß Feature Engineering**\n",
    "- [x] **Categorical Encoding**: Label encoding and One-hot encoding\n",
    "- [x] **Feature Creation**: Age groups, BMI, efficiency metrics, performance gaps\n",
    "- [x] **Polynomial Features**: Interaction terms between numerical variables\n",
    "- [x] **Domain Features**: Sports-specific calculated features\n",
    "\n",
    "### **üìâ Dimensionality Reduction**\n",
    "- [x] **PCA**: Linear dimensionality reduction with variance analysis\n",
    "- [x] **t-SNE**: Non-linear reduction for visualization\n",
    "- [x] **Comparison**: PCA vs t-SNE trade-offs and use cases\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights üîç\n",
    "\n",
    "1. **Missing Data**: KNN imputation preserves relationships better than simple statistics\n",
    "2. **Outliers**: FIFA dataset has natural outliers (superstars) that should be kept\n",
    "3. **Scaling**: Different algorithms prefer different scaling methods\n",
    "4. **Transformations**: Log transformation significantly reduced skewness in Value column\n",
    "5. **Feature Engineering**: Created 5+ new meaningful features from existing data\n",
    "6. **Dimensionality**: Can retain 95% of variance with ~6 components (from 10 original)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps üìà\n",
    "\n",
    "Preprocessing pipeline is complete! Ready for:\n",
    "- **Stage 3**: Data Mining (clustering, association rules, pattern discovery)\n",
    "- **Stage 4**: Modeling & Interpretation (ML models with preprocessed features)\n",
    "\n",
    "Great foundation for advanced analytics! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Best Practices Learned\n",
    "\n",
    "- Always handle missing values before scaling\n",
    "- Visualize data transformations to verify effectiveness\n",
    "- Use domain knowledge for feature engineering\n",
    "- Consider the end model when choosing preprocessing techniques\n",
    "- Document preprocessing steps for reproducibility"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
